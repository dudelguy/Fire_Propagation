{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructing wildfire propagation - An example\n",
    "\n",
    "This notebook is a step-by-step guide towards the reconstruction of wildfire propagation by combining a burned area database and active fire detection points. Multiple methods were sequentially applied to pre-process the two datasets, combine their inputs and create individual raster files, which contain the fire propgation intervals. If necessary, the starting point of the fire can be approximated as well. \n",
    "\n",
    "In this example, the EFFIS burned area database is used. It is free to download from the website of the [European Forest Fire Information System (EFFIS)](https://forest-fire.emergency.copernicus.eu/applications/data-and-services). \n",
    "The database is provided in an shp-format and includes wildfire events from different countries in Europe, North Africa, and the Middle East. It ranges back to the year 2000, where 13 countries started to provide the necessary information. \n",
    "The number of participating countries grew steadily over the years, and today, 50 countries contribute the data of wildfire events happening in their habitat. \n",
    "Every fire event can be located with a specific ID and its start- and end-date is given according to the information provided by the countries authorities. \n",
    "Since wildfires recordings depend on national authorities and ministries, information on start- and end-date can be of varying accuracy. \n",
    "\n",
    "The pre-processed active fire product from the [VIIRS sensor](https://www.earthdata.nasa.gov/learn/find-data/near-real-time/firms/viirs-i-band-375-m-active-fire-data) was used to include the active fire detection points. This data is provided by NASAs Fire Information for Resource Management System (FIRMS) and can be [downloaded](https://firms.modaps.eosdis.nasa.gov/download/) in varying formats (in this example, the csv-format is used) for different years, countries or self-defined areas. \n",
    "Other active fire products can be used as well, and especially MODIS is often applied to detect thermal anomalies all over the world. The relevant MODIS product can also be downloaded from the linked website. \n",
    "\n",
    "In this example, fire propagation is recunstructed for all wildfires in Italy in the year 2019 (or more specfic: all events from this year that are recoded in the EFFIS database). Thereto, the specific burned areas are selected from the EFFIS database and combined with VIIRS active fire detection data from the same year. The resulting fire propagation images are saved as .tif-files in a specified output folder. To code along, the necessary data needs to be downloaded first.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the necessary libraries\n",
    "\n",
    "While importing most necessary libraries can be done normally (e.g., using conda-forge and a specified conda environment), GDAL is used in geopandas, rasterio and rioxarray, which can sometimes lead to conflicts. This can be circumvented by following a specific order of installation. In the presented example, we used the Python Version 3.11.9 and installed the aforementioned packages in the following order:\n",
    "1. conda install -c conda-forge gdal\n",
    "2. conda install -c conda-forge geopandas rasterio\n",
    "3. conda install -c conda-forge rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import geopandas as geo\n",
    "import xarray\n",
    "import rioxarray as rio\n",
    "from osgeo import osr, ogr, gdal\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio import Affine, features\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import MergeAlg\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon, Point, mapping, box\n",
    "from scipy.interpolate import griddata\n",
    "import scipy.ndimage as ndi\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "import datetime\n",
    "import concurrent\n",
    "import ee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import the developed modules for calculating fire propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fire_prop import active_fire_detection, fire_progression, artifacts, check_fire_progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import burned area database\n",
    "\n",
    "Several columns from the burned area geodataframe are necessary to run the functions that reconstruct fire propagation. Depending on the burned area dataset that is used, the relevant columns can have different names. These names need to be set during class initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the burned area database (in shp-format) as a geopandas dataframe\n",
    "effis_burnt_area = geo.read_file(\".../modis.ba.poly.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EFFIS dataset for burned areas includes the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effis_burnt_area.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important columns include the start- and end-date of a fire, as well as the column that includes the ID of the fire polygon. The fire dates are needed to find the corresponding active fire detections, the ID is used to reference the resulting raster files to their specific fire event.\n",
    "Depending on the dataset, only the start-date is recorded, in which case the same column is used for both, start and end of the fire. \n",
    "\n",
    "Concerning the EFFIS database, the relevant columns are named 'FIREDATE', 'LASTUPDATE'and 'id'. The column names are saved as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set important column names of the EFFIS data\n",
    "col_name_start_fire = 'FIREDATE'\n",
    "col_name_end_fire = 'LASTUPDATE'\n",
    "col_name_id = 'id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid any problems concerning date conventions, the date formats are unified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify the date columns of start- and end-date in the dataframe\n",
    "effis_burnt_area[col_name_start_fire] = pd.to_datetime(effis_burnt_area[col_name_start_fire], format='mixed')\n",
    "effis_burnt_area[col_name_end_fire] = pd.to_datetime(effis_burnt_area[col_name_end_fire], format='mixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column that includes the country is also saved as a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_country = 'COUNTRY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Import active fire detection points\n",
    "\n",
    "Similar to the burned area database, several columns are necessary to run the functions of the 'active_fire_detection' class. Investigating the imported data is therefore an important first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the VIIRS active fire detection file (in .csv-format) for Italy and the year 2019 as a pandas dataframe\n",
    "viirs_sel_country_and_year = pd.read_csv('.../viirs-snpp_2019_Italy.csv')\n",
    "# Print the head of the dataframe\n",
    "viirs_sel_country_and_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important columns of the active fire file include the acquisition time, the acquisition date, as well as the latitude and the longitude of the active fire detection points. \n",
    "\n",
    "Concerning VIIRS active fire detection, the relevant columns are named 'acq_time', 'acq_date'. 'latitude', and 'longitude'. The column names are saved as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set important column names of the VIIRS data\n",
    "acq_time = \"acq_time\"\n",
    "acq_date = \"acq_date\"\n",
    "latitude = \"latitude\"\n",
    "longitude = \"longitude\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Process active fire detection points\n",
    "\n",
    "The file with active fire detections includes all active fire detection points of the chosen country and the chosen year. It is therefore necessary to reduce the file to fit the selected burned area geographically and temporally. \n",
    "Thereto, a new instance of the active_fire_detection class needs to be initialized. It includes the necessary functions for processing the active fire detection data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new instance of the active_fire_detection using the formerly defined column names of both datasets\n",
    "act_fire = active_fire_detection(acq_time, acq_date, latitude, longitude, col_name_id, col_name_start_fire, col_name_end_fire)\n",
    "\n",
    "# Use modify_viirs to convert the active fire detection dataframe to a geopandas dataframe \n",
    "viirs_gdf = act_fire.modify_viirs(viirs_sel_country_and_year)\n",
    "\n",
    "# create subset of the EFFIS database that overlaps with the active fire detection file in time and space\n",
    "# start with time (i.e., year)\n",
    "effis_burnt_area_sel_year = effis_burnt_area[~((effis_burnt_area[col_name_start_fire] <= '2019-01-01') | (effis_burnt_area[col_name_start_fire] >= f'2019-12-31'))]\n",
    "# continue with space (i.e., country)\n",
    "effis_burnt_area_subset = effis_burnt_area_sel_year[(effis_burnt_area_sel_year[col_name_country] == 'IT')]\n",
    "effis_burnt_area_subset = effis_burnt_area_subset[~pd.isnull(effis_burnt_area_subset[col_name_start_fire])]\n",
    "\n",
    "# unify crs of both datasets\n",
    "effis_gdf_reprj = effis_burnt_area_subset.to_crs(3035)\n",
    "viirs_gdf_reprj = viirs_gdf.to_crs(3035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer all EFFIS burnt area polygons in which to search for active fire detections. The buffered geometry is added to the geopandas dataframe of EFFIS.\n",
    "Naturally, this buffer zone should match the resolution of the active fire detection sensor.\n",
    "Here, to match the VIIRS resolution, a 200m buffer zone was chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 200\n",
    "effis_gdf_reprj[\"buffer\"] = effis_gdf_reprj.buffer(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EFFIS wildfire database contains a lot of short (< 2 days) and/or small (<10ha) fires. \n",
    "Oftentimes, these fires cannot be detected with the available active fire detection sensors.\n",
    "Therefore, many fire polygons in the database do not have any matching points in the active fire file\n",
    "The following code finds all polygons for which active fire detection worked \n",
    "and creates a dataset of all fires that span multiple days\n",
    "save their ID for subsetting the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_dates = []\n",
    "fire_id = []\n",
    "start_date = []\n",
    "end_date = []\n",
    "all_dates = []\n",
    "for i in range(len(effis_gdf_reprj)):\n",
    "    \n",
    "    selected_poly = effis_gdf_reprj.iloc[i]\n",
    "    \n",
    "    # find_viirs_for_effis is used to search for active fire detections that match the selected polygon of the database\n",
    "    # Don't forget to set day_thresh according to your needs. For this database, 4 days were found to work well\n",
    "    pot_points_to_sel_fire = act_fire.find_viirs_for_effis(selected_poly, viirs_gdf_reprj, 4)\n",
    "    \n",
    "    if len(pot_points_to_sel_fire)>0:\n",
    "        \n",
    "        # process the found active fire points \n",
    "        pot_points_to_sel_fire = act_fire.combine_sat_paths(pot_points_to_sel_fire, 2, 3)          \n",
    "        \n",
    "        # save metadata of the wildfire polygons of the database for which a active fire detection worked \n",
    "        different_dates.append(len(np.unique(pot_points_to_sel_fire[\"time_in_seconds\"])))\n",
    "        fire_id.append(selected_poly[\"id\"])\n",
    "        start_date.append(min(pot_points_to_sel_fire[\"datetime_grouped\"]))\n",
    "        end_date.append(max(pot_points_to_sel_fire[\"datetime_grouped\"]))\n",
    "        all_dates.append(np.unique(pot_points_to_sel_fire[\"datetime_grouped\"]))\n",
    "        \n",
    "# rearange the date format for later\n",
    "all_dates_new = []\n",
    "for k in range(len(all_dates)):\n",
    "    dates_lists = all_dates[k]\n",
    "    sel_measurement = []\n",
    "    for i in range(len(dates_lists)):\n",
    "        sel_measurement.append(str(dates_lists[i])[0:16].replace('-','_').replace(':','_') + '_00')\n",
    "             \n",
    "    all_dates_new.append(sel_measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the caclualted information, the EFFIS database can be reduced to only include fires that have active fire detection points for at least two consecutive dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index for all fire polygons of the database that were detected at at least two different dates \n",
    "idx_dates = (np.where(np.array(different_dates) > 1)[0])\n",
    "fire_idx_dates = np.array(fire_id)[idx_dates]\n",
    "\n",
    "# select all fire polygons using the index\n",
    "effis_to_use = act_fire.select_subset_using_id(effis_burnt_area_subset, fire_idx_dates)\n",
    "effis_to_use = effis_to_use.to_crs(3035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset is supposed to include equally sized raster files, since most use cases require equally sized images. Therefore, a new gepopandas dataframe with empty but equally sized rectangles is created. Every rectangle centers around a fire event. To restrict the size of the rectangles (and with it the size that is needed to save the data physically on a disk), width and height of the rectangle are adapted. This can be done according to the maximum fire size of all fires, or it can be set to not exceed a specific fire size.\n",
    "In this example, we set the maximum size of the rectangle according to the largest fires below a size of 10'000 ha. Not restricting the can cause the size of Sentinel-2 data to explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_extends = effis_to_use.bounds\n",
    "\n",
    "polygon_extends['width'] = np.abs(polygon_extends[\"maxx\"] - polygon_extends[\"minx\"])\n",
    "polygon_extends['height'] = np.abs(polygon_extends[\"maxy\"] - polygon_extends[\"miny\"])\n",
    "polygon_extends['centroid'] = effis_to_use.centroid\n",
    "polygon_extends['burned_ha'] = effis_to_use.area/10000\n",
    "polygon_extends['country'] = effis_to_use[col_name_country]\n",
    "polygon_extends['date'] = (effis_to_use[col_name_start_fire]).astype(str)\n",
    "polygon_extends['id'] = effis_to_use[col_name_id]\n",
    "polygon_extends['start_date'] = (np.array(start_date)[np.isin(np.array(fire_id),effis_to_use[col_name_id])]).astype(str)\n",
    "polygon_extends['end_date'] = (np.array(end_date)[np.isin(np.array(fire_id),effis_to_use[col_name_id])]).astype(str)\n",
    "\n",
    "# according to copernicus (https://climate.copernicus.eu/esotc/2021/wildfires), wildfire > 10'000 ha are 'critical fires'\n",
    "# these are removed here\n",
    "polygon_extends = polygon_extends[polygon_extends['burned_ha']<=10000]\n",
    "\n",
    "print(f\"number of fires smaller than 10000ha: {len(polygon_extends)}\")\n",
    "print(f\"maximum width of investigated fires: {max(polygon_extends['width'])}\")\n",
    "print(f\"maximum hight of investigated fires: {max(polygon_extends['height'])}\")\n",
    "\n",
    "# create length and height of all the equally sized polygons\n",
    "width_to_centroid = (max(polygon_extends['width']) / 2) + 150\n",
    "heigth_to_centroid = (max(polygon_extends['height']) / 2) + 150 \n",
    "# create new geodataframe with the center of every fire polygon\n",
    "polygon_extends_gdf = geo.GeoDataFrame(\n",
    "                            polygon_extends, geometry=polygon_extends[\"centroid\"], crs=\"EPSG:3035\"\n",
    "                        )\n",
    "\n",
    "# subset the effis dataset again, according to the thresholds in fire size\n",
    "effis_to_use = act_fire.select_subset_using_id(effis_to_use, polygon_extends_gdf[\"id\"])\n",
    "\n",
    "# with the centroid, create the rectangular, equally sized geometries for every fire polygon\n",
    "max_width_new_polygon = polygon_extends_gdf.geometry.x + width_to_centroid\n",
    "min_width_new_polygon = polygon_extends_gdf.geometry.x - width_to_centroid\n",
    "\n",
    "max_height_new_polygon = polygon_extends_gdf.geometry.y + heigth_to_centroid\n",
    "min_height_new_polygon = polygon_extends_gdf.geometry.y - heigth_to_centroid\n",
    "\n",
    "# create the final geodataframe that contains all equally sized rectangles\n",
    "to_poly =[]\n",
    "for i in range(len(polygon_extends_gdf)):\n",
    "    # Create a rectangular polygon using Shapely's box function\n",
    "    rectangular_polygon = box(min_width_new_polygon.iloc[i], min_height_new_polygon.iloc[i], max_width_new_polygon.iloc[i], max_height_new_polygon.iloc[i])\n",
    "    to_poly.append(rectangular_polygon)\n",
    "gdf_poly = geo.GeoDataFrame(polygon_extends, geometry = to_poly, crs=3035)\n",
    "gdf_poly = gdf_poly.drop([\"centroid\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create the final fire propagation dataset\n",
    "\n",
    "In the final step, fire propagation is reconstructed based on the burned area polygons and the pre-processed active fire detection points of the selected fire. For every individual acquisition time of the sensor that captures the active fires, a raster file with the respective burned area propagation at the time of acquisition is calculated and saved to disk (in the tif-format). Width and height of all raster files are taken from the formerly created geodandas dataframe. They are linked by using the unique id of every fire event. \n",
    "\n",
    "Before the raster files of an event are saved, potential artifacts are removed and the propagation is checked for plausibility. Additionally, the starting point of a fire is approximated and saved in a new geodataframe. The starting point has the same id as the corresponding fire event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an empty geodataframes, in which \n",
    "final_points_std = geo.GeoDataFrame()\n",
    "\n",
    "# initialize new instances of the classes that are necessary to calculate fire progression, \n",
    "# remove progression artifacts, and check for plausible fire propagation \n",
    "fire_to_process = fire_progression('time_in_seconds', 'geometry')\n",
    "rem_art = artifacts('time_in_seconds')\n",
    "check_fire_prog = check_fire_progression()\n",
    "\n",
    "# specify the output path. The resulting raster files will be saved in this path (in the tif-format)\n",
    "out_path = \"C:/Users/smuelle4/treeads/creating_reference/fire_propagation_tifs_test/italy\"\n",
    "\n",
    "# iterate over all fire events that have at least two different active fire detection dates\n",
    "# for every event, fire propagation is recunstruced individually\n",
    "for i in range(len(gdf_poly)):\n",
    "    \n",
    "    # the burned area of every event is selected iteratively from the reduced burned area database\n",
    "    selected_rectangle = gdf_poly.iloc[i]\n",
    "    selected_poly = effis_gdf_reprj[(effis_gdf_reprj[col_name_id] == selected_rectangle['id'])].iloc[0]\n",
    "    \n",
    "    # find_viirs_for_effis is used to search for active fire detections that match the selected polygon of the database\n",
    "    # day_thresh needs to be set according to your needs. The defined number of days are substracted/added to the start-/end-date of your fire event.\n",
    "    # This is necessary, because the dates reported in the burned area database are often not perfectly accurate, \n",
    "    # mostly underestimating the start- and end-date of a fire event. \n",
    "    # For the EFFIS database, 4 days worked best\n",
    "    pot_points_to_sel_fire = act_fire.find_viirs_for_effis(selected_poly,viirs_gdf_reprj,4)\n",
    "    \n",
    "    if len(pot_points_to_sel_fire)>0:\n",
    "        \n",
    "        # process the active fire detection points that are linked to the selected burned area\n",
    "        pot_points_to_sel_fire = act_fire.combine_sat_paths(pot_points_to_sel_fire, 2, 3)     \n",
    "\n",
    "        if len(np.unique(pot_points_to_sel_fire[\"time_in_seconds\"])) > 1:\n",
    "            \n",
    "            # fill_poly_with_hulls is used to reconstruct fire propagation  \n",
    "            data_xr_clip = fire_to_process.fill_poly_with_hulls(selected_poly, pot_points_to_sel_fire, effis_to_use.crs)\n",
    "\n",
    "            # these functions are used to remove potential artifacts induced by the k-nearest neighbor algorithm\n",
    "            data_xr_clip = rem_art.remove_single_pixel(data_xr_clip)\n",
    "            data_xr_clip = rem_art.check_for_islands(data_xr_clip)\n",
    "            data_xr_clip = rem_art.rem_artifacts(data_xr_clip, pot_points_to_sel_fire, 150)\n",
    "            \n",
    "            if len(np.unique(data_xr_clip))>1:\n",
    "                \n",
    "                # fire_prog_with_iterations is used to check for plausible fire propagation#\n",
    "                # if initial_fire_cluster = 0, fire propagation is not plausible\n",
    "                # if initial_fire_cluster = 1, fire propagation is plausible and exactly one starting area exists\n",
    "                # if initial_fire_cluster > 1, fire propagation is plausible, but multiple starting areas exist. \n",
    "                # In this case, the largest area is used to approximate the starting point\n",
    "                fire_progr, initial_fire_cluster = check_fire_prog.fire_prog_with_iterations(data_xr_clip,3)\n",
    "\n",
    "                # if fire propagation is plausible (i.e. initial_fire_cluster >= 1), raster files are saved for every date of fire propagation.\n",
    "                # the number initial_fire_cluster\n",
    "                if len(initial_fire_cluster)>=1:\n",
    "                    \n",
    "                    # find the largest starting area\n",
    "                    size_cluster = []\n",
    "                    for k in range(len(initial_fire_cluster)):\n",
    "                        size_cluster.append(len(np.where(initial_fire_cluster[k]==True)[0]))   \n",
    "                    \n",
    "                    # get all the individual fire propagation dates    \n",
    "                    unique_seconds = np.unique(data_xr_clip)[~np.isnan(np.unique(data_xr_clip))]\n",
    "                    dates_lists = np.unique(pot_points_to_sel_fire[\"datetime_grouped\"].iloc[np.where(np.isin(pot_points_to_sel_fire[\"time_in_seconds\"], unique_seconds))[0]])\n",
    "\n",
    "                    # convert every date object to a string and unify the format \n",
    "                    dates_str = []\n",
    "                    for i in range(len(dates_lists)):\n",
    "                        dates_str.append(str(dates_lists[i])[0:16].replace('-','_').replace(':','_') + '_00')    \n",
    "                             \n",
    "                    # approximate the ignition point and add it to the geodataframe                         \n",
    "                    ign_std = fire_to_process.calc_ignition_point(data_xr_clip, initial_fire_cluster[np.argmax(size_cluster)])\n",
    "                    for_starting_points_std = fire_to_process.create_new_point(selected_poly, ign_std[0], ign_std[1], 3035)\n",
    "                    final_points_std = pd.concat([final_points_std,for_starting_points_std])\n",
    "                    \n",
    "                    # fire_prog_raster_ident_size iteratively saves one raster for every step of fire propagation in the defined output path\n",
    "                    fire_to_process.fire_prog_raster_ident_size(data_xr_clip, selected_rectangle, 30, 3035, dates_str, out_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
